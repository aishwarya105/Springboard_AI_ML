{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boilerplate functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure that tensorflow works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15475461441367312750\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 14145241158911348193\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 11078030580949651462\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 145358848\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 8761194200305454774\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import numpy as np\n",
    "import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pylab as pylab\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 22.  28.]\n",
      " [ 49.  64.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "with tf.device('/gpu:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print (sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 22.  28.]\n",
      " [ 49.  64.]]\n"
     ]
    }
   ],
   "source": [
    "# Creates a graph.\n",
    "a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "c = tf.matmul(a, b)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(c))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as k\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Input, Dense, Lambda, Dropout, Convolution2D, MaxPooling2D, Flatten,Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train File :', ['__header__', '__globals__', 'fold_indices', 'y', 'X', '__version__', 'class_names'])\n",
      "('Test File :', ['__header__', '__globals__', 'y', 'X', '__version__', 'class_names'])\n"
     ]
    }
   ],
   "source": [
    "train_file = loadmat('../STL10/stl10_matlab/train.mat')\n",
    "test_file = loadmat('../STL10/stl10_matlab/test.mat')\n",
    "\n",
    "print(\"Train File :\" ,train_file.keys())\n",
    "print(\"Test File :\",test_file.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the train and test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train_images = train_file['X'] #5000 training images ((5000, 27648))\n",
    "# test_images = test_file['X'] #8000 test images ((8000, 27648))\n",
    "# print(test_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The images need to be reshaped for VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_reshaped = np.transpose(np.reshape(train_images,(-1,3,96,96)),(0,3,2,1))\n",
    "#-1 lets numpy determine the size on its own\n",
    "test_images_reshaped = np.transpose(np.reshape(test_images,(-1,3,96,96)),(0,3,2,1))\n",
    "\n",
    "print(train_images_reshaped.shape)\n",
    "print(test_images_reshaped.shape)\n",
    "\n",
    "print(train_images_reshaped[5].shape) #images is 96x96 with 3 colored channels\n",
    "imshow(train_images_reshaped[5])\n",
    "plt.show()\n",
    "imshow(test_images_reshaped[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_images,test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the test image set because it causes memory errors during pre-processing\n",
    "test_images_reshaped_1 = test_images_reshaped[:4000]\n",
    "test_images_reshaped_2 = test_images_reshaped[4000:]\n",
    "del test_images_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(train_images_reshaped[5])\n",
    "plt.show()\n",
    "imshow(test_images_reshaped_1[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, tarfile, time\n",
    "from PIL import Image\n",
    "\n",
    "def resize_tensorimages(X_raw, interp_method=Image.BICUBIC):\n",
    "    # VGG16 accepts input images of size (224, 224, 3)\n",
    "    # Therefore, convert X_train_raw and X_test_raw to PIL.Image format, \n",
    "    # then resize the images from (96, 96, 3) to (224, 224, 3) \n",
    "    \n",
    "    # X_224 = np.zeros((X_raw.shape[0], 224, 224, 3)) --this takes up too much RAM\n",
    "    X_224 = [] #creates a list\n",
    "    for i, _ in enumerate(X_raw):\n",
    "        im = Image.fromarray(np.uint8(X_raw[i]))\n",
    "        img = im.resize((224, 224), interp_method)\n",
    "        arr = np.array(img)\n",
    "        X_224.append(arr)\n",
    "    \n",
    "    return np.asarray(X_224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train  = resize_tensorimages(train_images_reshaped)\n",
    "# del train_images_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_1 = resize_tensorimages(test_images_reshaped_1)\n",
    "# del test_images_reshaped_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_2 = resize_tensorimages(test_images_reshaped_2)\n",
    "# del test_images_reshaped_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the VGG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "#---------------------------------------------------------\n",
    "\n",
    "model_bottom = VGG16(include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess training images for VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocess_input(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = model_bottom.predict(X_train[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "(2, 7, 7, 512)\n",
      "<type 'int'>\n"
     ]
    }
   ],
   "source": [
    "print type(d)\n",
    "print d.shape\n",
    "print type(d.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features = np.zeros((X_train.shape[0],d.shape[1],d.shape[2],d.shape[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:  2.56%\n",
      "Training set:  5.12%\n",
      "Training set:  7.68%\n",
      "Training set: 10.24%\n",
      "Training set: 12.80%\n",
      "Training set: 15.36%\n",
      "Training set: 17.92%\n",
      "Training set: 20.48%\n",
      "Training set: 23.04%\n",
      "Training set: 25.60%\n",
      "Training set: 28.16%\n",
      "Training set: 30.72%\n",
      "Training set: 33.28%\n",
      "Training set: 35.84%\n",
      "Training set: 38.40%\n",
      "Training set: 40.96%\n",
      "Training set: 43.52%\n",
      "Training set: 46.08%\n",
      "Training set: 48.64%\n",
      "Training set: 51.20%\n",
      "Training set: 53.76%\n",
      "Training set: 56.32%\n",
      "Training set: 58.88%\n",
      "Training set: 61.44%\n",
      "Training set: 64.00%\n",
      "Training set: 66.56%\n",
      "Training set: 69.12%\n",
      "Training set: 71.68%\n",
      "Training set: 74.24%\n",
      "Training set: 76.80%\n",
      "Training set: 79.36%\n",
      "Training set: 81.92%\n",
      "Training set: 84.48%\n",
      "Training set: 87.04%\n",
      "Training set: 89.60%\n",
      "Training set: 92.16%\n",
      "Training set: 94.72%\n",
      "Training set: 97.28%\n",
      "Training set: 99.84%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import division\n",
    "batch_size = 128\n",
    "\n",
    "for i in range(len(X_train)//batch_size):\n",
    "    idx = range(i*batch_size, (i+1)*batch_size)\n",
    "    print('Training set: %5.2f%%' % ((i+1)/(len(X_train)/batch_size)*100))\n",
    "    X_train_features[idx] = model_bottom.predict(X_train[idx])    \n",
    "X_train_features[(i+1)*batch_size:] = model_bottom.predict(X_train[(i+1)*batch_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../STL10/data/X_train_features',X_train_features)\n",
    "del X_train_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing test images for VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_1 = preprocess_input(X_test_1)\n",
    "X_test_2 = preprocess_input(X_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_features_1 = np.zeros((X_test_1.shape[0],7,7,512))\n",
    "X_test_features_2 = np.zeros((X_test_2.shape[0],7,7,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set 1:  3.20%\n",
      "Test set 1:  6.40%\n",
      "Test set 1:  9.60%\n",
      "Test set 1: 12.80%\n",
      "Test set 1: 16.00%\n",
      "Test set 1: 19.20%\n",
      "Test set 1: 22.40%\n",
      "Test set 1: 25.60%\n",
      "Test set 1: 28.80%\n",
      "Test set 1: 32.00%\n",
      "Test set 1: 35.20%\n",
      "Test set 1: 38.40%\n",
      "Test set 1: 41.60%\n",
      "Test set 1: 44.80%\n",
      "Test set 1: 48.00%\n",
      "Test set 1: 51.20%\n",
      "Test set 1: 54.40%\n",
      "Test set 1: 57.60%\n",
      "Test set 1: 60.80%\n",
      "Test set 1: 64.00%\n",
      "Test set 1: 67.20%\n",
      "Test set 1: 70.40%\n",
      "Test set 1: 73.60%\n",
      "Test set 1: 76.80%\n",
      "Test set 1: 80.00%\n",
      "Test set 1: 83.20%\n",
      "Test set 1: 86.40%\n",
      "Test set 1: 89.60%\n",
      "Test set 1: 92.80%\n",
      "Test set 1: 96.00%\n",
      "Test set 1: 99.20%\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "batch_size = 128\n",
    "\n",
    "for i in range(len(X_test_1)//batch_size):\n",
    "    idx = range(i*batch_size, (i+1)*batch_size)\n",
    "    print('Test set 1: %5.2f%%' % ((i+1)/(len(X_test_1)/batch_size)*100))\n",
    "    X_test_features_1[idx] = model_bottom.predict(X_test_1[idx])    \n",
    "X_test_features_1[(i+1)*batch_size:] = model_bottom.predict(X_test_1[(i+1)*batch_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../STL10/data/X_test_features_1',X_test_features_1)\n",
    "del X_test_features_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set 2:  3.20%\n",
      "Test set 2:  6.40%\n",
      "Test set 2:  9.60%\n",
      "Test set 2: 12.80%\n",
      "Test set 2: 16.00%\n",
      "Test set 2: 19.20%\n",
      "Test set 2: 22.40%\n",
      "Test set 2: 25.60%\n",
      "Test set 2: 28.80%\n",
      "Test set 2: 32.00%\n",
      "Test set 2: 35.20%\n",
      "Test set 2: 38.40%\n",
      "Test set 2: 41.60%\n",
      "Test set 2: 44.80%\n",
      "Test set 2: 48.00%\n",
      "Test set 2: 51.20%\n",
      "Test set 2: 54.40%\n",
      "Test set 2: 57.60%\n",
      "Test set 2: 60.80%\n",
      "Test set 2: 64.00%\n",
      "Test set 2: 67.20%\n",
      "Test set 2: 70.40%\n",
      "Test set 2: 73.60%\n",
      "Test set 2: 76.80%\n",
      "Test set 2: 80.00%\n",
      "Test set 2: 83.20%\n",
      "Test set 2: 86.40%\n",
      "Test set 2: 89.60%\n",
      "Test set 2: 92.80%\n",
      "Test set 2: 96.00%\n",
      "Test set 2: 99.20%\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "batch_size = 128\n",
    "\n",
    "for i in range(len(X_test_2)//batch_size):\n",
    "    idx = range(i*batch_size, (i+1)*batch_size)\n",
    "    print('Test set 2: %5.2f%%' % ((i+1)/(len(X_test_2)/batch_size)*100))\n",
    "    X_test_features_2[idx] = model_bottom.predict(X_test_2[idx])    \n",
    "X_test_features_2[(i+1)*batch_size:] = model_bottom.predict(X_test_2[(i+1)*batch_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../STL10/data/X_test_features_2',X_test_features_2)\n",
    "del X_test_features_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the train and test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_file['y']\n",
    "test_labels = test_file['y']\n",
    "class_labels = test_file['class_names']\n",
    "# print(train_labels.shape) #5000 labels for 5000 images ((5000,1))\n",
    "# print(min(train_labels),max(train_labels)) #class labels go from 1-10.\n",
    "\n",
    "\n",
    "#Let's re-adjust the labels to be from 0 to 9\n",
    "train_labels_adjusted = train_labels - 1\n",
    "test_labels_adjusted = test_labels - 1\n",
    "# print(min(train_labels_adjusted),max(train_labels_adjusted)) #Class labels adjuested goes from 0 to 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_file,test_file\n",
    "del train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print type(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array([u'airplane'],\n",
      "      dtype='<U8')\n",
      "  array([u'bird'],\n",
      "      dtype='<U4') array([u'car'],\n",
      "      dtype='<U3')\n",
      "  array([u'cat'],\n",
      "      dtype='<U3') array([u'deer'],\n",
      "      dtype='<U4')\n",
      "  array([u'dog'],\n",
      "      dtype='<U3') array([u'horse'],\n",
      "      dtype='<U5')\n",
      "  array([u'monkey'],\n",
      "      dtype='<U6') array([u'ship'],\n",
      "      dtype='<U4')\n",
      "  array([u'truck'],\n",
      "      dtype='<U5')]]\n"
     ]
    }
   ],
   "source": [
    "print np.asarray(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(train_labels_adjusted, num_classes = 1000)\n",
    "y_test  = to_categorical(test_labels_adjusted , num_classes = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_labels_adjusted, test_labels_adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)#remember this was 5000x1 earlier and to categorical changed this to one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing stuff to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# y_train[0][:100]\n",
    "# y_temp = np.argmax(y_train,axis = 1)\n",
    "# print(y_train[1][:100]) #5th index is 1\n",
    "# print(y_temp[1])# this will store the index 5 after argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "10\n",
      "<type 'numpy.ndarray'>\n",
      "500\n",
      "(10, 500)\n"
     ]
    }
   ],
   "source": [
    "# # print(set(y_temp)) #there are 10 classes in our dataset\n",
    "# idx = []\n",
    "# for cls in set(y_temp):\n",
    "#     idx.append(np.where(y_temp==cls)[0])\n",
    "# print(type(idx))#list\n",
    "# print(len(idx))#list of 10 classes\n",
    "# print(type(idx[0]))#each list element is an array\n",
    "# print(len(idx[0]))# each class has 500 numbers\n",
    "# idx = np.asarray(idx)\n",
    "# print(idx.shape)#convert to 10x500 array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_KFold(y_onehot, n_split=5):\n",
    "    y = np.argmax(y_onehot, axis=1)\n",
    "    #axis 1 looks at all columns within the row\n",
    "    #so for each image we get the max index\n",
    "    # input y: numpy array of class ID's\n",
    "    \n",
    "    idx = []\n",
    "    for cls in set(y):\n",
    "        # idx[cls] is the indices to class no cls\n",
    "        # Idx will store indices where each class label is true\n",
    "        # so 1st 500 idx will have indices where class label is 0, next 500 will be for class label 1 and so on\n",
    "        idx.append(np.where(y==cls)[0])\n",
    "        \n",
    "    idx = np.asarray(idx)\n",
    "    for i in range(len(idx)):#will go from 0 to 9 for 10 classes\n",
    "        np.random.shuffle(idx[i]) #shuffle the images within each class\n",
    "\n",
    "    fidx = []\n",
    "    fold_width = int(idx.shape[1]/n_split)# no. of images per class divided by number of folds\n",
    "    \n",
    "    for k in range(n_split):\n",
    "        out = []\n",
    "        \n",
    "        #get kth fold for each class\n",
    "        for i in range(len(idx)):#loops 10 times *same as number of class*\n",
    "            out.append(idx[i,k*fold_width : (k+1)*fold_width])\n",
    "        \n",
    "        #combine the kth fold of 10 classes and shuffle it\n",
    "        out = np.concatenate(out)\n",
    "        np.random.shuffle(out)\n",
    "        \n",
    "        #store the the kth fold in fidx\n",
    "        fidx.append(out)\n",
    "    \n",
    "    fidx = np.array(fidx)\n",
    "    np.random.shuffle(fidx)\n",
    "    return fidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnames_fidx(y_onehot, n_split=5):\n",
    "    # Returns\n",
    "    # fidx: fold indices on the training set (0,...4999)\n",
    "    # cnames: class names (10 classes)\n",
    "    \n",
    "    cnames = class_labels\n",
    "\n",
    "    fidx = custom_KFold(y_onehot, n_split=n_split)\n",
    "    \n",
    "    print('Class names retrieved in cnames')\n",
    "    print('K-Fold indices generated, n_split = %i'%(n_split))\n",
    "    print('Fold width = %i'%(fidx.shape[1]))\n",
    "    print('Number of data points for each class = %i'%(fidx.shape[1]/n_split))\n",
    "    return cnames, fidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class names retrieved in cnames\n",
      "K-Fold indices generated, n_split = 5\n",
      "Fold width = 1000\n",
      "Number of data points for each class = 200\n",
      "All labeled data loaded.\n"
     ]
    }
   ],
   "source": [
    "cnames, fidx = get_cnames_fidx(y_train, n_split=5)\n",
    "print('All labeled data loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_features1 = np.load('../STL10/data/X_test_features_1.npy')\n",
    "X_test_features2 = np.load('../STL10/data/X_test_features_2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_features =  np.concatenate([X_test_features1,X_test_features2])\n",
    "del X_test_features1,X_test_features2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features = np.load('../STL10/data/X_train_features.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-1e843df52082>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mX_train_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_all = np.concatenate([X_train_features,X_test_features])\n",
    "del X_train_features,X_test_features\n",
    "\n",
    "y_all = np.concatenate([y_train, y_test])\n",
    "del y_train,y_test\n",
    "\n",
    "idx = list(range(13000))\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "X_train_features = X_all[idx[:5000]]\n",
    "X_test_features = X_all[idx[5000:]]\n",
    "del X_all\n",
    "\n",
    "y_train = y_all[idx[:5000]]\n",
    "y_test = y_all[idx[5000:]]\n",
    "del y_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('X_test_features', 1605632144),\n",
       " ('X_train_features', 1003520144),\n",
       " ('y_test', 32000112),\n",
       " ('y_train', 20000112),\n",
       " ('idx', 117120),\n",
       " ('fidx', 40112),\n",
       " ('Activation', 904),\n",
       " ('Conv2D', 904),\n",
       " ('Convolution2D', 904),\n",
       " ('Dense', 904),\n",
       " ('Dropout', 904),\n",
       " ('Flatten', 904),\n",
       " ('Lambda', 904),\n",
       " ('MaxNLocator', 904),\n",
       " ('MaxPooling2D', 904),\n",
       " ('Model', 904),\n",
       " ('RMSprop', 904),\n",
       " ('Sequential', 904),\n",
       " ('Input', 120),\n",
       " ('custom_KFold', 120),\n",
       " ('get_cnames_fidx', 120),\n",
       " ('imshow', 120),\n",
       " ('loadmat', 120),\n",
       " ('to_categorical', 120),\n",
       " ('train_test_split', 120),\n",
       " ('class_labels', 112),\n",
       " ('cnames', 112),\n",
       " ('a', 64),\n",
       " ('b', 64),\n",
       " ('c', 64),\n",
       " ('sess', 64),\n",
       " ('device_lib', 56),\n",
       " ('k', 56),\n",
       " ('np', 56),\n",
       " ('plt', 56),\n",
       " ('pylab', 56),\n",
       " ('tf', 56)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# These are the usual ipython objects, including this one you are creating\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# Get a sorted list of the objects and their sizes\n",
    "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_model():\n",
    "    from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "    from keras.layers import Dropout, Flatten, Dense\n",
    "    from keras.models import Sequential\n",
    "\n",
    "    model_top = Sequential() \n",
    "    model_top.add(GlobalAveragePooling2D(data_format='channels_last', input_shape=(7, 7, 512))) \n",
    "    # model_top.add(Dense(1000, activation='relu')) \n",
    "    model_top.add(Dense(1000, activation='relu')) \n",
    "    model_top.add(Dense(1000, activation='softmax')) \n",
    "\n",
    "\n",
    "    model_top.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy']) \n",
    "\n",
    "    return model_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(xlabel=None, ylabel=None, title=None, xlim=None, ylim=None):\n",
    "    fig = plt.figure()\n",
    "    ax = plt.gca() # gets current axis\n",
    "    fig.set_size_inches((15,5))\n",
    "    plt.grid('on')\n",
    "    if xlabel: plt.xlabel(xlabel)\n",
    "    if ylabel: plt.ylabel(ylabel)\n",
    "    if xlim: plt.xlim((0, xlim));\n",
    "    if ylim: plt.ylim((0, ylim));\n",
    "    if title: plt.title(title)\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_top, \n",
    "                X_train, y_train,\n",
    "                X_cv, y_cv,\n",
    "                epochs=20,\n",
    "                batch_size=128,\n",
    "                wanna_plot=False,\n",
    "                fpath='model'):\n",
    "    \n",
    "    start = time.time()\n",
    "    from keras.callbacks import ModelCheckpoint  \n",
    "    print('Initiate Training....')\n",
    "    # ..................................................................\n",
    "    \n",
    "    callback_inst = Callback_Func((X_train, y_train),(X_cv, y_cv), start, wanna_plot=wanna_plot)\n",
    "\n",
    "    checkpointer = ModelCheckpoint(filepath='../STL10/data/saved_models/' + fpath + '.best.hdf5', \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    model_top.fit(X_train, y_train, \n",
    "              validation_data=(X_cv, y_cv),\n",
    "              epochs=epochs, \n",
    "              batch_size=batch_size, \n",
    "              callbacks=[callback_inst, checkpointer], \n",
    "              verbose=0)\n",
    "\n",
    "    print_runtime(start)\n",
    "\n",
    "    return model_top, callback_inst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "class Callback_Func(Callback):\n",
    "    def __init__(self, train_data, test_data, start, wanna_plot=True):\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.loss_train = []\n",
    "        self.loss_test = []\n",
    "        self.acc = []\n",
    "        self.start = start\n",
    "        self.wanna_plot = wanna_plot\n",
    "        \n",
    "        \n",
    "    def plotter(self, title='validation accuracy'):\n",
    "        ax = plt.subplot(121)\n",
    "        plt.xlabel('epochs')\n",
    "        plt.grid('on')\n",
    "        plt.title('loss')\n",
    "        x_plot = range(1, len(self.loss_train)+1)\n",
    "        plt.xlim((1,max(max(x_plot),2)))\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        ax.plot(x_plot, self.loss_test, 'r--^', alpha=.7, label=\"validation\")\n",
    "        ax.plot(x_plot, self.loss_train, 'k--^', alpha=.7, label=\"train\")\n",
    "        plt.legend()\n",
    "\n",
    "        ax = plt.subplot(122)\n",
    "        plt.xlabel('epochs')\n",
    "        plt.grid('on')\n",
    "        ax.plot(x_plot, self.acc, 'b--^', alpha=.5)\n",
    "        plt.xlim((1,max(max(x_plot),2)))\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        plt.title(title)\n",
    "        plt.ion()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        X_train, y_train = self.train_data\n",
    "        X_test, y_test = self.test_data\n",
    "        _loss_train, _ = self.model.evaluate(X_train, y_train, batch_size=1024, verbose=0)\n",
    "        _loss_test, _acc = self.model.evaluate(X_test, y_test, batch_size=1024, verbose=0)\n",
    "        self.loss_train.append(_loss_train)\n",
    "        self.loss_test.append(_loss_test)\n",
    "        self.acc.append(_acc)\n",
    "        \n",
    "        \n",
    "        if self.wanna_plot: \n",
    "            self.plotter()\n",
    "            end = time.time()\n",
    "            print('\\nRuntime: %d min %d sec' % ((end-self.start)//60, (end-self.start)%60))\n",
    "        \n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_runtime(start):\n",
    "    end = time.time()\n",
    "    print('Runtime: %d min %d sec' % ((end-start)//60, (end-start)%60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_3 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1000)              513000    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1000)              1001000   \n",
      "=================================================================\n",
      "Total params: 1,514,000\n",
      "Trainable params: 1,514,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Initiate Training....\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Dst tensor is not initialized.\n\t [[{{node _arg_global_average_pooling2d_3_input_0_0}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-376a32563dfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m                                            \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                                            \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                                            wanna_plot=False)\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mmodel_top\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../STL10/data/saved_models/model.best.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-114-2a0cb029e96a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_top, X_train, y_train, X_cv, y_cv, epochs, batch_size, wanna_plot, fpath)\u001b[0m\n\u001b[1;32m     22\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcallback_inst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpointer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m               verbose=0)\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint_runtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aishwarya105/anaconda3/envs/my-test-env/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/aishwarya105/anaconda3/envs/my-test-env/lib/python2.7/site-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    215\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                             \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aishwarya105/anaconda3/envs/my-test-env/lib/python2.7/site-packages/keras/callbacks.pyc\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-115-9e9e4fe37784>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0m_loss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0m_loss_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_loss_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aishwarya105/anaconda3/envs/my-test-env/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1111\u001b[0m                                          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                                          \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1113\u001b[0;31m                                          steps=steps)\n\u001b[0m\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m     def predict(self, x,\n",
      "\u001b[0;32m/home/aishwarya105/anaconda3/envs/my-test-env/lib/python2.7/site-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aishwarya105/anaconda3/envs/my-test-env/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aishwarya105/anaconda3/envs/my-test-env/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aishwarya105/anaconda3/envs/my-test-env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aishwarya105/anaconda3/envs/my-test-env/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.pyc\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Dst tensor is not initialized.\n\t [[{{node _arg_global_average_pooling2d_3_input_0_0}}]]"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "acc = []\n",
    "\n",
    "# ### Do a 5 k-fold\n",
    "# from sklearn.model_selection import KFold\n",
    "# kf = KFold(n_splits=5, shuffle=True)\n",
    "# for fidx_train, fidx_cv in kf.split(train_labels_adjusted):\n",
    "    \n",
    "    \n",
    "for k, _ in enumerate(fidx):\n",
    "    fidx_cv = fidx[k]\n",
    "    fidx_train = np.concatenate([fidx[0:k],fidx[k+1:]]).flatten()\n",
    "    \n",
    "    X_cv_features_1k = X_train_features[fidx_cv]\n",
    "    y_cv_1k = y_train[fidx_cv]\n",
    "    \n",
    "    X_train_features_9k = X_train_features[fidx_train]\n",
    "    y_train_9k = y_train[fidx_train]\n",
    "    \n",
    "    \n",
    "    model_top = construct_model()\n",
    "    if k == 0: model_top.summary()\n",
    "        \n",
    "    model_top, callback_inst = train_model(model_top, \n",
    "                                           X_train_features_9k, y_train_9k,\n",
    "                                           X_cv_features_1k, y_cv_1k, \n",
    "                                           epochs=10,\n",
    "                                           batch_size=128,\n",
    "                                           wanna_plot=False)\n",
    "    \n",
    "    model_top.load_weights('../STL10/data/saved_models/model.best.hdf5')\n",
    "    _loss, _acc = model_top.evaluate(X_test_features, y_test,batch_size=128);\n",
    "    \n",
    "    acc.append(_acc)\n",
    "    print('\\nAt k = %i, \\nTest Accuracy = %.2f %%' % (k, _acc*100))\n",
    "\n",
    "    ### Plot loss function and accuracy\n",
    "    callback_inst.plotter()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "#     ### 7.  Compute test accuracy.\n",
    "\n",
    "#     _loss, _acc = model_top.evaluate(X_test_features, y_test, \n",
    "#                                     batch_size=128);\n",
    "#     acc.append(_acc)\n",
    "#     print('\\nAt k = %i, \\nTest Accuracy = %.2f %%' % (k, _acc*100))\n",
    "\n",
    "#     ### Plot loss function and accuracy\n",
    "#     callback_inst.plotter()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------------------------\n",
      "------------------------------\n",
      "In 5 Folds\n",
      "K-Fold Average Test Accuracy = nan %  <==\n",
      "Runtime: 1 min 18 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aishwarya105/anaconda3/envs/my-test-env/lib/python2.7/site-packages/numpy/core/fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/aishwarya105/anaconda3/envs/my-test-env/lib/python2.7/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(('\\n' + ('\\n'+'-'*30)*2 +'\\nIn %i Folds'+ '\\nK-Fold Average Test Accuracy = %.2f %%  <==') % \n",
    "      (len(fidx), np.mean(acc)*100))\n",
    "print_runtime(start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-test-env",
   "language": "python",
   "name": "my-test-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
